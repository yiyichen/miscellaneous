{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Phrase Extraction #\n",
    "This notebook explores techniques for effectively extracting keyphrases from texts. A few methods are explored, with code, sample output, commentary and some experiments along the way. If you'd like to see the final result, jump directly to the end of the notebook.\n",
    "\n",
    "\n",
    "The novel 1984 (nineteen eighty-four) is used as a test text collection for this exploration.\n",
    "\n",
    "\n",
    "\n",
    "## Pre-Processing ##\n",
    "Load and process the text collection to get it ready for key phrase extraction.  \n",
    "\n",
    "### Import Libraries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Content ###\n",
    "\n",
    "For testing only: I am using the novel 1984 (nineteen eighty-four) as my text collection to test out the different techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the contents of text collection into a string in memory\n",
    "with open('1984.rtf') as f:\n",
    "    test_content = f.read()\n",
    "\n",
    "# Pre-process to clean up the content\n",
    "test_content = test_content.replace(\"\\\\par\", \"\")\n",
    "test_content = test_content.replace(\"&\", \"and\")\n",
    "test_content = test_content[171:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the desirable output, please replace \"news.txt\" with the target content source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_source = 'mystery_text_expository_2016.txt'\n",
    "with open(target_source) as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize ###  \n",
    "Tokenize the text collection by sentences, which will help us do POS tagging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break down the content into a list of sentences\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sents = sent_tokenizer.tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> HELPER FUNCTIONS <<<<<<<<<<<<<<<<<<<<\n",
    "# Tokenize each sentence\n",
    "def word_tokenize2(sents):    \n",
    "    pattern = r'''\n",
    "      ([A-Z]\\.)+            # abbreviations, e.g. U.S.A.\n",
    "    | \\w+'\\w+               # contractions, e.g. I'm\n",
    "    | \\w+(-\\w+)*            # words with optional internal hyphens\n",
    "    | \\$?\\d+(\\.\\?d+)?%?     # currency and percentages, e.g. $12.40, 82%\n",
    "    | \\.\\.\\.                # ellipsis\n",
    "    | [][.,;\"'?():-_`]      # these are separate tokens; includes ], [\n",
    "    '''\n",
    "    \n",
    "    matchIterators = [re.finditer(pattern, sent, re.VERBOSE) for sent in sents]\n",
    "    \n",
    "    # tokensBySent is a list of list where each list contains tokens in a sentence\n",
    "    # tokens is a list of all tokens in the content\n",
    "    tokensBySent = []\n",
    "    tokens = []\n",
    "    \n",
    "    for matchIterator in matchIterators:\n",
    "        words = [match.group(0) for match in matchIterator]\n",
    "        tokensBySent.append(words)\n",
    "        tokens += words\n",
    "        \n",
    "    return tokensBySent, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokensBySent, tokens = word_tokenize2(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a words-only version of the tokenized collection for frequent terms analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all numbers and symbols\n",
    "# wordsBySent is a list of list where each list contains words in a sentence\n",
    "# words is a list of all words in the content\n",
    "wordsBySent = [[token for token in tokens if re.match(\"[a-zA-Z]+\", token)] for tokens in tokensBySent]\n",
    "words = [token for token in tokens if re.match(\"[a-zA-Z]+\", token)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS (Part of Speech) Tagging ###\n",
    "Use the nltk default tagger to tag the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use a pre-trained tagger to tag the text collection\n",
    "tagged_tokens = [nltk.tag.pos_tag(token) for token in tokensBySent]\n",
    "tagged_sents = [nltk.tag.pos_tag(sent) for sent in wordsBySent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Interesting Words ... By Frequency? ##\n",
    "#### a.k.a Showing Frequent Terms ####\n",
    "\n",
    "What does an interesting word look like in a text collection? It probably occurs in the text just the right amount of times - less frequently than a stopword, but more often than just once or twice (so it is unlikely to be a typo or tangential thought) \n",
    "\n",
    "Words that are too frequent and/or short are filtered out, and then the most common 20 words are shown.  \n",
    "Filtering criteria:  \n",
    "1. Filter out all numbers and punctuations\n",
    "2. Filter out all stop words\n",
    "3. Filter out all words of length <= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# interestingWords is a list of normalized words where stopwords and very short words are removed\n",
    "# removing stopwords (both lower and upper-case)\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "interestingWords = [word for word in words if word.lower() not in cachedStopWords]\n",
    "\n",
    "# lowercasing and only showing words of a greater length\n",
    "interestingWords = [word for word in interestingWords if len(word) >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Congress', 2382),\n",
       " ('people', 2240),\n",
       " ('Government', 2095),\n",
       " ('States', 1999),\n",
       " ('world', 1711),\n",
       " ('United', 1701),\n",
       " ('American', 1639),\n",
       " ('would', 1614),\n",
       " ('years', 1481),\n",
       " ('great', 1434),\n",
       " ('country', 1399),\n",
       " ('peace', 1106),\n",
       " ('every', 1099),\n",
       " ('public', 1074),\n",
       " ('America', 1025),\n",
       " ('Federal', 998),\n",
       " ('national', 981),\n",
       " ('government', 938),\n",
       " ('power', 902),\n",
       " ('shall', 898)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see some interesting words!\n",
    "freqdist = nltk.FreqDist(interestingWords)\n",
    "freqdist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start seeing a little bit of content there, but nothing particularly interesting... We can maybe increase the number of words we show. However, without context it is difficult to infer what the text is about, as most of these words can take on multiple meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Nouns and Beyond ##\n",
    "#### a.k.a Information obtained from Syntax or Partial Parsing (Chunking) ####\n",
    "\n",
    "### Proper Nouns ###\n",
    "\n",
    "What about nouns? If we can see what objects are in the text, maybe it will give us a better idea of what the text is about?  \n",
    "\n",
    "I explored that idea by extracting the most frequent proper nouns using chunking and displaying them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> HELPER FUNCTIONS <<<<<<<<<<<<<<<<<<<<\n",
    "# Get the chunked text list using the given chunker specified by the grammar string and the label\n",
    "# the grammar string and the label should contain the same keyword, e.g. 'PROPER_NOUN'\n",
    "def get_chunked_text(grammar, tagged_sents, labels):\n",
    "    # chunk parser (cp) for proper nouns\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    chunked_text = []\n",
    "    for sent in tagged_sents:\n",
    "        if len(sent) > 0:\n",
    "            for subtree in cp.parse(sent).subtrees():\n",
    "                if subtree.label() in labels:\n",
    "                    chunked_text += [text for (text, _) in subtree.leaves()]\n",
    "    return chunked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Congress', 2382),\n",
       " ('Government', 1966),\n",
       " ('States', 1792),\n",
       " ('United', 1701),\n",
       " ('America', 1025),\n",
       " ('Federal', 879),\n",
       " ('Americans', 552),\n",
       " ('State', 548),\n",
       " ('President', 496),\n",
       " ('Department', 489),\n",
       " ('Union', 439),\n",
       " ('National', 355),\n",
       " ('Secretary', 319),\n",
       " ('Commission', 288),\n",
       " ('Navy', 285),\n",
       " ('Europe', 275),\n",
       " ('War', 272),\n",
       " ('Army', 256),\n",
       " ('Act', 254),\n",
       " ('American', 245)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunk parser (cp) for proper nouns\n",
    "grammar_NP = r\"\"\" PROPER_NOUN:\n",
    "    {<NNP.*>+}           \n",
    "    \"\"\"\n",
    "proper_nouns = get_chunked_text(grammar_NP, tagged_sents, 'PROPER_NOUN')\n",
    "nltk.FreqDist(proper_nouns).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to see more interesting words, e.g. \"Newspeak\", \"Police\", but there's still words like \"I'm\", \"Inner\" that are ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Entities ###\n",
    "\n",
    "How can we do better? What if we take a look into the special nouns (name entity words)? They might give us a better idea of the topics of the text.  \n",
    "\n",
    "To get a meaningful list of name entities, I first parsed the text collection using the default nltk ne chunker to get a preliminary list. I then tried to deduplicate the list by combining similar words. The 20 most common name entities are then shown.\n",
    "\n",
    "** Point of Reflection **  \n",
    "Before the deduplication process, I decided to filter out name entity words occuring only once from the preliminary list. The goal for this step is to take out words that are incorrectly tagged and/or peripheral, thereby reducing noise in the deduplication step. Some key terms only occur once in a text collection, and this step unfortunately will remove them from the result. By trying it both ways, I feel that the filtering does more good than harm. I therefore choose to keep it and sacrifice recall for a better precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> HELPER FUNCTIONS <<<<<<<<<<<<<<<<<<<<\n",
    "# Get a list of name entities with their occurence frequency from a list of tagged sentences\n",
    "# input: a list of POS tagged sentences\n",
    "# output: frequency distribution of (name entity, their label)\n",
    "def get_name_entities(tagged_sents):\n",
    "    # chunk collection using the default nltk ne chunker\n",
    "    chunked_sents = [nltk.ne_chunk(sent) for sent in tagged_sents]\n",
    "    \n",
    "    name_entities = []\n",
    "    \n",
    "    # extract name entities from each sentence\n",
    "    for sent in chunked_sents:\n",
    "        for subtree in sent.subtrees():\n",
    "            \n",
    "            # nltk ne chunker returns three types of entities: GPE, ORGANIZATION, and PERSON\n",
    "            # we want to retrieve words in any of the three categories\n",
    "            if subtree.label() in ['GPE', 'ORGANIZATION', 'PERSON']:\n",
    "                \n",
    "                # if the subtree contains more than one token, combine the tokens together into one\n",
    "                name_entity = ' '.join([word for (word, _) in subtree.leaves()])\n",
    "                name_entities += [(name_entity, subtree.label())]\n",
    "    \n",
    "    # create a frequency distribution of the name entities\n",
    "    return nltk.FreqDist(name_entities)\n",
    "\n",
    "# Compare two entities and  \n",
    "# 1. if they are similar, return the entity that best represents both entities\n",
    "# 2. if they are dissimilar, return None\n",
    "# entities a and b have the format ((word, name entity type), frequency)\n",
    "def get_common_terms(a, b):\n",
    "    a_term_list = a[0][0].split()\n",
    "    b_term_list = b[0][0].split()\n",
    "    \n",
    "    term = None\n",
    "    sum_freq = a[1] + b[1]\n",
    "    \n",
    "    if len(a_term_list) > 1 and len(b_term_list) > 1:\n",
    "        similarity_score = SequenceMatcher(None, a[0][0], b[0][0]).ratio()     \n",
    "        if similarity_score >= 0.75: term = a[0] if a[1] > b[1] else b[0]\n",
    "    elif len(a_term_list) > 1 and b_term_list[0] in a_term_list: term = a[0]\n",
    "    elif len(b_term_list) > 1 and a_term_list[0] in b_term_list: term = b[0]\n",
    "    else:\n",
    "        similarity_score = SequenceMatcher(None, a[0][0], b[0][0]).ratio()\n",
    "        if similarity_score >= 0.9:\n",
    "            term = a[0] if a[1] > b[1] else b[0]\n",
    "    \n",
    "    if term is None: return None\n",
    "    else: return (term, sum_freq)\n",
    "\n",
    "# Take a name entity list, and consolidate similar terms into one term with combined frequency, e.g.\n",
    "# if the word 'Richard' appeared 12 times and 'Richard Thomas' appeared 20 times in a text,\n",
    "# the function removes 'Richard' from the list, and only keeps 'Richard Thomas' with an updated frequency of 32\n",
    "def dedup_ne_list(ne_list):\n",
    "    new_ne_list = []\n",
    "    while len(ne_list) > 0:\n",
    "        term1 = ne_list.pop()\n",
    "        last_common_term = term1\n",
    "        i = 0\n",
    "        while i < len(ne_list):\n",
    "            term2 = ne_list[i]\n",
    "            common_term = get_common_terms(last_common_term, term2)\n",
    "            if common_term is not None:\n",
    "                ne_list.remove(term2)\n",
    "                last_common_term = common_term\n",
    "            else: \n",
    "                i += 1\n",
    "        new_ne_list += [last_common_term]\n",
    "    \n",
    "    new_ne_list = sorted([term for term in new_ne_list if term[1] > 1], key = lambda x: x[1], reverse = True)\n",
    "    return new_ne_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get preliminary name entity list\n",
    "freqdist = get_name_entities(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('National Congress', 'ORGANIZATION'), 2496),\n",
       " (('American Army', 'ORGANIZATION'), 1731),\n",
       " (('United States', 'GPE'), 1520),\n",
       " (('Latin America', 'PERSON'), 925),\n",
       " (('Federal Housing Administration', 'ORGANIZATION'), 532),\n",
       " (('State Department', 'ORGANIZATION'), 526),\n",
       " (('Applause', 'ORGANIZATION'), 385),\n",
       " (('Eastern Europe', 'GPE'), 249),\n",
       " (('Senate', 'ORGANIZATION'), 235),\n",
       " (('Nation', 'ORGANIZATION'), 233),\n",
       " (('Mainland China', 'GPE'), 223),\n",
       " (('Treasury Department', 'ORGANIZATION'), 212),\n",
       " (('Americans', 'GPE'), 209),\n",
       " (('Republic of Korea', 'ORGANIZATION'), 208),\n",
       " (('Navy', 'ORGANIZATION'), 204),\n",
       " (('George Washington', 'PERSON'), 177),\n",
       " (('Arab States', 'GPE'), 171),\n",
       " (('Cuba', 'GPE'), 171),\n",
       " (('House', 'ORGANIZATION'), 166),\n",
       " (('United Nations', 'ORGANIZATION'), 151)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out terms that only occur once to reduce noise\n",
    "ne_freq_list = [(term, freqdist[term]) for term in freqdist if freqdist[term] > 1]\n",
    "\n",
    "# deduplicate the list and return the top 20 words\n",
    "new_ne_list = dedup_ne_list(ne_freq_list.copy())\n",
    "new_ne_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> OUTPUT <<<<<<<<<<<<<<<<<<<<\n",
    "# create output string for name entity result\n",
    "ne_output = [(ne[0][1], ne[0][0]) for ne in new_ne_list[:20]]\n",
    "ne_output_string = ''\n",
    "for ne in ne_output:\n",
    "    ne_output_string += ne[1] + ', '\n",
    "ne_output_string = ne_output_string[:-2]+ '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now seen some key 'players' in the text. What are they doing? How are they related to each other? We explore that by looking into other aspects of the text. One way to get an overall understanding of the context of the text is to look into the concept space of the words in text, and see where they overlap.\n",
    "\n",
    "Let's take a look at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## High Level Concepts ##\n",
    "#### a.k.a Using Semantic Similarity / Finding Higher-Level Concepts ####\n",
    "\n",
    "I used WordNet to navigate through the high level concepts of the text collection.  \n",
    "\n",
    "1. For each word of interest in the text collection, its hypernyms are extracted and added into an overall list along with hypernyms of other words.  \n",
    "2. For each hypernym in the overall list, I calculated their frequency and specificity.\n",
    "3. The hypernyms are then assigned a score = - (1/frequency\\* + 1/specificity\\*), and the hypernyms with the 20 highest scores are shown.\n",
    "\n",
    "*Notes*  \n",
    "A substantial portion of the code is adopted from code by Anna Swigart, ANLP 2015  \n",
    "\\* adjusted by +1 to avoid 1/0 error\n",
    "\n",
    "** Point of Reflection **  \n",
    "So we have a list of hypernyms, how do we know which ones are the most useful in telling us the context of the text? I think there are two intuitive and easy to measure metrics - the frequency and specificity of the hypernym. If more words share the same hypernym, it is likely to be more representative of the context of the text. Additionally, if they hypernym is very specific, then it gives us more detailed information about the text. The next question is then how to rank the hypernyms based on their frequency and specificity. My first approach was to put hard limit on the values of the two metrics, i.e. throwing out a hypernym if its frequency and/or specificity are lower than a threshold. Finding that threshold, however, was a challenge since it is almost an arbitrary decision, and really depends on the specific text collection. I then decided to use a relative score (~ sum of inverse of frequency and specificity), calculate that for each hypernym, rank them by the score, and output the ones with the highest score. The second approach avoids the need for an arbitrary external threshold on the metrics, and relies on the relative usefulness of each hypernym against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> HELPER FUNCTIONS <<<<<<<<<<<<<<<<<<<<\n",
    "# adopted from code by Anna Swigart, ANLP 2015\n",
    "\n",
    "# get n most frequent normalized unigrams of word_type (N for noun and V for verb) in tagged sentences\n",
    "# if n is None, return full set of normalized unigrams of word_type\n",
    "# word processing done for normalization: lemmatization, removing stop words, punctuation, removing short words\n",
    "def get_unigrams(tagged_sents, word_type, n):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_sents\n",
    "                           for word in sent \n",
    "                           if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           and word[0] not in punctuation \n",
    "                           and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           and len(word[0])>= 5\n",
    "                           and word[1].startswith(word_type)]\n",
    "    if n is None: \n",
    "        normed_unigrams = set(normed_tagged_words)\n",
    "    else:\n",
    "        normed_unigrams = [word for (word, count) in nltk.FreqDist(normed_tagged_words).most_common(n)]\n",
    "    return normed_unigrams\n",
    "\n",
    "# get frequency distribution of hypernyms of tokens of given word_type in tagged sentences\n",
    "# first the top normalized unigrams are extraced from the tagged sentences\n",
    "# then for each unigram, add its list of hypernyms and add it to an overall hypernym list\n",
    "# count the occurences of each hypernym and return a frequency distribution\n",
    "def categories_from_hypernyms(sents, word_type, n = None):\n",
    "    \n",
    "    # We can specify a fixed length termlist by only using the n most frequent unigrams\n",
    "    # I tested both and found very minimal difference between the two, so I decided to\n",
    "    # use the full set\n",
    "    termlist = get_unigrams(sents, word_type, n)\n",
    "    \n",
    "    hypterms_dict = defaultdict(list)\n",
    "    for term in termlist:\n",
    "        s = wn.synsets(term.lower(), word_type.lower())\n",
    "        for syn in s:\n",
    "            for hyp in syn.hypernyms():\n",
    "                hypterms_dict[hyp.name].append(term)\n",
    "    return hypterms_dict\n",
    "\n",
    "# get the hypernyms and their frequencies from a given hypernym dictionary, \n",
    "# and for each hypernym, add information about its specificity, as defined by the \n",
    "# depth (min_depth) of its corresponding synset\n",
    "# return the information as a list of triplets (hypernym, depth, frequencies / occurences)\n",
    "def get_hypernym_info(hypernyms_dict):\n",
    "    new_hypernyms_dict = []\n",
    "    for hypernym in hypernyms_dict.keys():\n",
    "        new_hypernyms_dict += [(hypernym(), \n",
    "                                wn.synset(hypernym()).min_depth(), \n",
    "                                len(hypernyms_dict.get(hypernym)))]\n",
    "    \n",
    "    # sort the list by specificity, i.e. the min depth of the specific hypernym\n",
    "    new_hypernyms_dict = sorted(new_hypernyms_dict, key = lambda x: x[1], reverse = True)\n",
    "    return new_hypernyms_dict\n",
    "\n",
    "# returns a string with the names of hypernyms in a hypernym list\n",
    "def print_hypernym(hypernyms_list):\n",
    "    hypernyms = ''\n",
    "    pattern = r\"([a-zA-Z]+?)\\.\"\n",
    "    for triplet in hypernyms_list:\n",
    "        match = re.search(pattern, wn.synset(triplet[0]).name())\n",
    "        hypernyms += match.group(1) + ', '\n",
    "    return hypernyms[:-2]+ '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbs ### \n",
    "Let's take a look at the action words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate hypernym dictionary for the given tagged text \n",
    "verb_hypernyms_dict = categories_from_hypernyms(tagged_tokens, 'V')\n",
    "\n",
    "# create a list of (hypernym, specificity, frequency) for hypernyms in the dictionary\n",
    "verb_hypernyms = get_hypernym_info(verb_hypernyms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('challenge.v.02', 8, 28),\n",
       " ('call.v.05', 8, 24),\n",
       " ('express.v.01', 7, 35),\n",
       " ('command.v.02', 8, 18),\n",
       " ('mean.v.01', 7, 23),\n",
       " ('forbid.v.01', 9, 14),\n",
       " ('order.v.01', 7, 20),\n",
       " ('convey.v.01', 6, 30),\n",
       " ('protest.v.02', 6, 24),\n",
       " ('rede.v.02', 6, 22),\n",
       " ('request.v.02', 6, 22),\n",
       " ('provoke.v.03', 9, 10),\n",
       " ('hash_out.v.01', 5, 38),\n",
       " ('restrict.v.03', 5, 38),\n",
       " ('urge.v.01', 7, 13),\n",
       " ('publicize.v.01', 5, 32),\n",
       " ('broach.v.01', 6, 17),\n",
       " ('guarantee.v.04', 7, 12),\n",
       " ('imply.v.02', 8, 10),\n",
       " ('elaborate.v.01', 6, 15)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank hypernyms by - [1/(freq + 1) + 1/(specificity + 1)]\n",
    "verb_hypernyms_filtered = sorted(verb_hypernyms, key = lambda x: - (1/(x[1]+1) + 1/(x[2]+1)), reverse = True)\n",
    "\n",
    "# output the top 20 hypernyms\n",
    "verb_hypernyms_filtered[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> OUTPUT <<<<<<<<<<<<<<<<<<<<\n",
    "# create output string for verb high level concept result\n",
    "verb_hypernyms_output = verb_hypernyms_filtered[:20]\n",
    "verb_hypernyms_output_string = print_hypernym(verb_hypernyms_output)\n",
    "verb_hypernyms_output_synset = [wn.synset(triplet[0]) for triplet in verb_hypernyms_filtered[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns ### \n",
    "How about some nouns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate hypernym dictionary for the given tagged text \n",
    "noun_hypernyms_dict = categories_from_hypernyms(tagged_tokens, 'N')\n",
    "\n",
    "# create a list of (hypernym, specificity, frequency) for hypernyms in the dictionary\n",
    "noun_hypernyms = get_hypernym_info(noun_hypernyms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('common_fraction.n.01', 9, 22),\n",
       " ('termination.n.05', 8, 29),\n",
       " ('beginning.n.05', 8, 22),\n",
       " ('improvement.n.02', 8, 22),\n",
       " ('motion.n.06', 7, 29),\n",
       " ('decrease.n.04', 8, 20),\n",
       " ('executive_department.n.01', 11, 12),\n",
       " ('increase.n.05', 8, 18),\n",
       " ('room.n.01', 7, 23),\n",
       " ('motion.n.03', 7, 22),\n",
       " ('production.n.07', 9, 13),\n",
       " ('support.n.10', 7, 20),\n",
       " ('position.n.06', 7, 20),\n",
       " ('device.n.01', 6, 32),\n",
       " ('large_integer.n.01', 6, 31),\n",
       " ('change.n.03', 6, 30),\n",
       " ('tract.n.01', 6, 29),\n",
       " ('blow.n.01', 9, 12),\n",
       " ('activity.n.01', 5, 91),\n",
       " ('payment.n.01', 7, 18)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank hypernyms by - [1/(freq + 1) + 1/(specificity + 1)]\n",
    "noun_hypernyms_filtered = sorted(noun_hypernyms, key = lambda x: - (1/(x[1]+1) + 1/(x[2]+1)), reverse = True)\n",
    "\n",
    "# output the top 20 hypernyms\n",
    "noun_hypernyms_filtered[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> OUTPUT <<<<<<<<<<<<<<<<<<<<\n",
    "# create output string for noun high level concept result\n",
    "noun_hypernyms_output = noun_hypernyms_filtered[:20]\n",
    "noun_hypernyms_output_string = print_hypernym(noun_hypernyms_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the high level concept method is working better on verbs than nouns, at least for the test text collection. Maybe nouns are more abmiguous by nature, which make their hypernyms tell a less coherent story?  \n",
    "\n",
    "So far our exploration has mostly revolved around information extraction from single words (except for chunking). While it gives us useful information, it is worth looking into words in relation to their neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Collocations and Friends ##\n",
    "#### a.k.a Collocations ####\n",
    "\n",
    "Let's start with bigrams and trigrams. I used the default nltk collocation bigram and trigram finder, and filtered out punctuations, stop words and infrequent terms. \n",
    "\n",
    "### Bigrams and Trigrams ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>> HELPER FUNCTIONS <<<<<<<<<<<<<<<<<<<<\n",
    "# get the n top bigrams using nltk default collocation bigram finder\n",
    "# punctuations, stop words and infrequent terms are filtered out\n",
    "# pmi scoring measure is used\n",
    "def bigram_collocations(words, n):\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_word_filter(lambda w: w[0] in punctuation)\n",
    "    finder.apply_word_filter(lambda w: w.lower() in cachedStopWords)\n",
    "    finder.apply_freq_filter(2)\n",
    "    return finder.nbest(bigram_measures.pmi, n)\n",
    "\n",
    "# get the n top trigrams using nltk default collocation bigram finder\n",
    "# punctuations, stop words and infrequent terms are filtered out\n",
    "# pmi scoring measure is used\n",
    "def trigram_collocations(words, n):\n",
    "    finder = nltk.collocations.TrigramCollocationFinder.from_words(words)\n",
    "    finder.apply_word_filter(lambda w: w[0] in punctuation)\n",
    "    finder.apply_word_filter(lambda w: w.lower() in cachedStopWords)\n",
    "    finder.apply_freq_filter(2)\n",
    "    return finder.nbest(trigram_measures.pmi, n)\n",
    "\n",
    "# return a formatted string of n-grams given a list of n-grams\n",
    "def print_ngrams(ngram_list):\n",
    "    ngrams = ''\n",
    "    for ngram in ngram_list:\n",
    "        ngrams += ' '.join(ngram) + ', '\n",
    "    return ngrams[:-2]+ '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bahia', 'Honda'),\n",
       " ('Border', 'Patrol'),\n",
       " ('Bretton', 'Woods'),\n",
       " ('Cabot', 'Lodge'),\n",
       " ('Chiang', 'Kai-shek'),\n",
       " ('Chris', 'Getsla'),\n",
       " ('Circular', 'Note'),\n",
       " ('Clear', 'Skies'),\n",
       " ('Crimes', 'Prevention'),\n",
       " ('Cycle', 'Evaluation'),\n",
       " ('Founding', 'Fathers'),\n",
       " ('Hate', 'Crimes'),\n",
       " ('Identic', 'Circular'),\n",
       " ('Jennifer', 'Rodgers'),\n",
       " ('Jet', 'Bomber'),\n",
       " ('Kevin', 'Jett'),\n",
       " ('La', 'Abra'),\n",
       " ('Lech', 'Walesa'),\n",
       " ('Leonard', 'Wood'),\n",
       " ('Liggett', 'Meyers')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 20 bigrams\n",
    "bigram_collocations(tokens, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hate', 'Crimes', 'Prevention'),\n",
       " ('Identic', 'Circular', 'Note'),\n",
       " ('Fuel', 'Cycle', 'Evaluation'),\n",
       " ('Generalissimo', 'Chiang', 'Kai-shek'),\n",
       " ('Range', 'Jet', 'Bomber'),\n",
       " ('Alien', 'Property', 'Custodian'),\n",
       " ('Outer', 'Continental', 'Shelf'),\n",
       " ('Receipts', 'tures', 'icit'),\n",
       " ('Henry', 'Cabot', 'Lodge'),\n",
       " ('Bretton', 'Woods', 'Agreements'),\n",
       " ('SUPPLEMENTAL', 'LEGISLATION', 'NEEDED'),\n",
       " ('INSULAR', 'POSSESSIONS', 'Conditions'),\n",
       " ('PUBLIC', 'DOMAIN', 'EBOOKS'),\n",
       " ('CUBAN', 'PARCEL', 'POST'),\n",
       " ('Roman', 'Catholic', 'Church'),\n",
       " ('POSTAL', 'SAVINGS', 'BANKS'),\n",
       " ('Consumer', 'Price', 'Index'),\n",
       " ('Interagency', 'Coal', 'Task'),\n",
       " ('Dean', 'C.', 'Worcester'),\n",
       " ('Martin', 'Luther', 'King')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 20 trigrams\n",
    "trigram_collocations(tokens, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigrams and trigrams showed us some new interesting information not seen from the previous sections. Unfortunately, some of the bigrams/trigrams are non-sensible, which undermine their usefulness as key phrases.  \n",
    "\n",
    "Can we try a different approach that incorporates what we have already known from the earlier analysis?\n",
    "\n",
    "### Attempt 1: Include Bigrams that are related to Name Entities ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interesting_entities is the list of top name entities extracted from the earlier section\n",
    "interesting_entities = [ne[1] for ne in ne_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('La', 'Abra'), ('Schedule', 'K')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment\n",
    "# take a list of top 200 bigrams, create a new bigram list \n",
    "# the new bigram list includes a bigram in the old list only if \n",
    "# at least one of its two terms is part of the top 20 name entities\n",
    "\n",
    "bigrams = bigram_collocations(tokens, 200)\n",
    "new_bigrams = []\n",
    "\n",
    "for bigram in bigrams:\n",
    "    entity_related = any(bigram[0] in entity for entity in interesting_entities) or any(\n",
    "        bigram[1] in entity for entity in interesting_entities)\n",
    "    if entity_related: new_bigrams += [bigram]\n",
    "\n",
    "new_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matching does not work very well. In addition, due to the partial matching restriction, much of the new information in the old bigram list is now lost.\n",
    "\n",
    "### Attempt 2: Identify 'Interesting' Sentences and Parse with Chunker ###\n",
    "If a sentence contains nouns that are part of the top 20 name entities, and at the same time contains verbs that have hypernyms in the top hypernyms list, that sentence might be of interest to us. I used the chunk parser to parse out the noun, proposition and verb part of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> HELPER FUNCTIONS <<<<<<<<<<<<<<<<<<<<\n",
    "# checks to see if a sentence contains a potentially important noun\n",
    "# returns True if the tagged sentence contains a noun word \n",
    "# that is in the list of interesting name entities\n",
    "# else return False\n",
    "def contains_entity(tagged_sent):\n",
    "    for (term, tag) in tagged_sent:\n",
    "        if tag.startswith('N') and term in interesting_entities:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# checks to see if a sentence contains a potentially important verb\n",
    "# returns True if the tagged sentence contains a verb word \n",
    "# that has a hypernym in the list of top verb hypernyms\n",
    "# else return False\n",
    "def concept_related(tagged_sent):\n",
    "    for (term, tag) in tagged_sent:\n",
    "        if tag.startswith('V'):\n",
    "            s = wn.synsets(term.lower(), 'v')\n",
    "            for syn in s:\n",
    "                if any(verb_hypernym in syn.hypernyms() for verb_hypernym in verb_hypernyms_output_synset):\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Experiment\n",
    "# for the first 100 sentences of the text, if a sentence evaluates to true for both \n",
    "# contains_entity and concept_related,\n",
    "# print out its parsed NP, PP and VP chunks\n",
    "\n",
    "# grammar for parsing out nouns, propositions and verbs\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  \"\"\"\n",
    "\n",
    "for sent in tagged_sents[:min(100, len(tagged_sents))]:\n",
    "    if concept_related(sent) and contains_entity(sent):\n",
    "        print()\n",
    "        print(get_chunked_text(grammar, [sent], 'NP'))\n",
    "        print(get_chunked_text(grammar, [sent], 'PP'))\n",
    "        print(get_chunked_text(grammar, [sent], 'VP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result does not look very promising for the first 100 sentences of the test text collection. The results are redundant and noisy, and confuses more than clarifies the context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Summary and Output ##\n",
    "Overall, methods that seem to produce most meaningful information:\n",
    "1. Name entities (chunking)\n",
    "2. High level concepts for verbs (semantic similarity)  \n",
    "<br>\n",
    "\n",
    "After that, some other methods that give somewhat useful information:\n",
    "1. High-level concepts for nouns (semantic similarity)\n",
    "2. Bigrams and trigrams (collocations)\n",
    "3. Interesting words by term frequencies (frequent terms) - This is not included in the final output as the list of words is mostly covered by the output of other methods   \n",
    "<br>\n",
    "\n",
    "The following attempts doesn't work with a naive approach, but might be worth exploring further:\n",
    "1. Determine the usefulness of bigrams/trigrams by referencing name entities\n",
    "2. Determine relevance of each sentence using name entities and high-level concepts and extract info accordingly  \n",
    "\n",
    "### Output ###\n",
    "Time to get all the pieces together in one place :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Entities of the Text Collection:\n",
      "National Congress, American Army, United States, Latin America, Federal Housing Administration, State Department, Applause, Eastern Europe, Senate, Nation, Mainland China, Treasury Department, Americans, Republic of Korea, Navy, George Washington, Arab States, Cuba, House, United Nations\n",
      "\n",
      "\n",
      "Key Actions of the Text Collection:\n",
      "challenge, call, express, command, mean, forbid, order, convey, protest, rede, request, provoke, out, restrict, urge, publicize, broach, guarantee, imply, elaborate\n",
      "\n",
      "\n",
      "---------- Other Words of Potential Interest ----------\n",
      "Some Concepts for Nouns:\n",
      "fraction, termination, beginning, improvement, motion, decrease, department, increase, room, motion, production, support, position, device, integer, change, tract, blow, activity, payment\n",
      "\n",
      "Some Bigrams:\n",
      "Bahia Honda, Border Patrol, Bretton Woods, Cabot Lodge, Chiang Kai-shek, Chris Getsla, Circular Note, Clear Skies, Crimes Prevention, Cycle Evaluation\n",
      "\n",
      "Some Trigrams:\n",
      "Hate Crimes Prevention, Identic Circular Note, Fuel Cycle Evaluation, Generalissimo Chiang Kai-shek, Range Jet Bomber, Alien Property Custodian, Outer Continental Shelf, Receipts tures icit, Henry Cabot Lodge, Bretton Woods Agreements\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# helper function to update the output with result\n",
    "# ouput = output + result if the new output has total length of less than 2000 characters\n",
    "def update_output(result, output, counter):\n",
    "    if counter < 2000:\n",
    "        result = result[:min(2000 - counter, len(result))]\n",
    "        output += result\n",
    "        counter = len(output)\n",
    "    return output, counter\n",
    "\n",
    "output, c = '', 0\n",
    "\n",
    "# Name entities with their types\n",
    "output, c = update_output('Key Entities of the Text Collection:\\n' + ne_output_string + '\\n', output, c)\n",
    "\n",
    "# Key verb concepts for the text collection\n",
    "output, c = update_output('Key Actions of the Text Collection:\\n' + verb_hypernyms_output_string + '\\n', output, c)\n",
    "\n",
    "output += '\\n---------- Other Words of Potential Interest ----------\\n'\n",
    "\n",
    "# Key noun concepts for the text collection\n",
    "output, c = update_output('Some Concepts for Nouns:\\n' + noun_hypernyms_output_string + '\\n', output, c)\n",
    "\n",
    "# Top bigrams by pmi measure\n",
    "output, c = update_output('Some Bigrams:\\n' + print_ngrams(bigram_collocations(tokens, 10)), output, c)\n",
    "\n",
    "# Top trigrams by pmi measure\n",
    "output, c = update_output('Some Trigrams:\\n' + print_ngrams(trigram_collocations(tokens, 10)), output, c)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
